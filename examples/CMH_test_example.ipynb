{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Audit-AI for Testing Adverse Impact over Time: An Introduction to Cochran-Mantel-Haenzel and Breslow-Day Statistics\n",
    "### Contributing data scientists: Anne Thissen-Roe & Lewis Baker\n",
    "\n",
    "The Audit-AI package provides developers with simple functions that can be used to test for adverse impact in an algorithm. Many of the original functions in this repo were for testing single instances of an algorithm. In application, however, it is often necessary to test for historic trends in a decision-making pipeline. This pipeline could be any number of decisions: the college admissions rate of men vs women over the past decade, the engine failure rate of six major car manufacturers for the past 18 months, or even the hourly number of ad clicks over time in two website formats during AB testing.\n",
    "\n",
    "Here we illustrate the pairwise special case of the Cochran-Mantel-Haenzel test. The CMH test is a generalization of the McNemar Chi-Squared test of Homogenaity. Whereas the McNemar test examines differences over two intervals (usually before and after), the CMH test examines differences over any number of _k_ instances.\n",
    "\n",
    "\n",
    "### A Note on Significance Testing for Adverse Impact\n",
    "\n",
    "Basic researchers often use the CMH test to identify significant differences between groups caused by an experimental condition. In this case, statisticians recommend the application of Yates' correction for continuity. This acts to reduce the test statistics and increase the p-value of tests to correct for false discovery rate at moderately large samples (recommended by some experts to include N's of tens to hundreds of datapoints). Yate's correction is a conservative approach in experimental settings, but NOT in adverse impact monitoring; such an approach would systematically allow marginal cases of bias to go undetected.\n",
    "\n",
    "Users are also advised to take practical significance into account. Statistical significance may be achieved at sufficiently large sample sizes despite trivially small effect sizes. Users should consult the industry, academic and regulatory norms for practical significance in their use-case.\n",
    "\n",
    "### Example: McDonald and Siebenaller (1989) \n",
    "\n",
    "Example taken from the Handbook of Biological Statistics by John H. McDonald (http://www.biostathandbook.com/cmh.html). From the text:\n",
    "\n",
    "    \"McDonald and Siebenaller (1989) surveyed allele frequencies at the Lap\n",
    "    locus in the mussel Mytilus trossulus on the Oregon coast. At four\n",
    "    estuaries, we collected mussels from inside the estuary and from a marine\n",
    "    habitat outside the estuary. There were three common alleles and a couple\n",
    "    of rare alleles; based on previous results, the biologically interesting\n",
    "    question was whether the Lap94 allele was less common inside estuaries,\n",
    "    so we pooled all the other alleles into a \"non-94\" class.\"\n",
    "\n",
    "    \"There are three nominal variables: allele (94 or non-94), habitat\n",
    "    (marine or estuarine), and area (Tillamook, Yaquina, Alsea, or Umpqua).\n",
    "    The null hypothesis is that at each area, there is no difference in the\n",
    "    proportion of Lap94 alleles between the marine and estuarine habitats.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import auditai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estuarine</th>\n",
       "      <th>Marine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>69</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>non-94</th>\n",
       "      <td>77</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Estuarine  Marine\n",
       "94             69      56\n",
       "non-94         77      40"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tillamook = pd.DataFrame({'Marine':[56,40],'Estuarine':[69,77]}, index=['94','non-94'])\n",
    "tillamook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[        Estuarine  Marine\n",
       " 94             69      56\n",
       " non-94         77      40,         Estuarine  Marine\n",
       " 94            257      61\n",
       " non-94        301      57,         Estuarine  Marine\n",
       " 94             65      73\n",
       " non-94         79      71,         Estuarine  Marine\n",
       " 94             48      71\n",
       " non-94         48      55]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yaquina = pd.DataFrame({'Marine':[61,57],'Estuarine':[257,301]}, index=['94','non-94'])\n",
    "alsea = pd.DataFrame({'Marine':[73,71],'Estuarine':[65,79]}, index=['94','non-94'])\n",
    "umpqua = pd.DataFrame({'Marine':[71,55],'Estuarine':[48,48]}, index=['94','non-94'])\n",
    "dfs = [tillamook,yaquina,alsea,umpqua]\n",
    "\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the CMH test from auditai\n",
    "from auditai.stats import test_cmh_bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common odds ratio: 1.31748487024\n",
      "CMH Chi-Squared Statistic: 5.32092776794\n",
      "CMH p-value: 0.0210707899383\n",
      "Breslow-Day Chi-Squared Statistic: 0.529485909032\n",
      "Breslow-Day p-value: 0.912367342097\n"
     ]
    }
   ],
   "source": [
    "# pass_col is reduntant here, as either case could be considered \"passing\" a priori\n",
    "# setting `pass_col` to False compares collumns in order (Estuarine compared to Marine)\n",
    "# Note that all statistics are identical, except `r`, the common odds ratio, \n",
    "# which will be inverted\n",
    "r, cmh, pcmh, bd, pbd = test_cmh_bd(dfs=dfs, pass_col=False) \n",
    "print \"common odds ratio: {}\".format(r)\n",
    "print \"CMH Chi-Squared Statistic: {}\".format(cmh)\n",
    "print \"CMH p-value: {}\".format(pcmh)\n",
    "print \"Breslow-Day Chi-Squared Statistic: {}\".format(bd)\n",
    "print \"Breslow-Day p-value: {}\".format(pbd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7590219991052517,\n",
       " 5.320927767938459,\n",
       " 0.02107078993834932,\n",
       " 0.5294859090315414,\n",
       " 0.9123673420971034)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cmh_bd(dfs=dfs, pass_col='Estuarine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining the Functions\n",
    "\n",
    "The `test_cmh_bd` function is a wrapper for three statistical analyses.\n",
    "\n",
    "\n",
    "### Odds ratio\n",
    "The `multi_odds_ratio` function computes the common odds ratio from average of classifications across multiple samples. The odds ratio of the total  _will_ diverge from the odds ratio at each sampling interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odds ratio at each sampling interval\n",
      "1.56231884058\n",
      "1.25339613626\n",
      "1.24962080173\n",
      "1.29090909091\n",
      "\n",
      "odds ratio for the total sample\n",
      "1.31748487024\n"
     ]
    }
   ],
   "source": [
    "from auditai.stats import multi_odds_ratio\n",
    "\n",
    "print 'odds ratio at each sampling interval'\n",
    "for df in dfs:\n",
    "    print multi_odds_ratio(df)\n",
    "\n",
    "print '\\nodds ratio for the total sample'\n",
    "print multi_odds_ratio(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interested parties will note that the common odds ratio of the total is not just the harmonic mean of the sample odds ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3390612173699998"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([1.56231884058,1.25339613626,1.24962080173,1.29090909091])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nor is the total common odds ratio just the odds ratio of the sum of all sample measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34636403567\n"
     ]
    }
   ],
   "source": [
    "dfTOT = dfs[0] + dfs[1] + dfs[2] + dfs[3]\n",
    "print multi_odds_ratio(dfTOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather, the total odds ratio is an approximation of the true, unknown odds ratio, taken from the ratio of \"pass\" and \"fail\" observations of both categories. Note that this method works even in the current case, where there is not a true pass or fail category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auditai.utils.cmh import parse_matrix, extract_data\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80.3550813286 60.9912744683\n",
      "1.31748487024\n"
     ]
    }
   ],
   "source": [
    "r_num = []\n",
    "r_den = []\n",
    "for df in dfs:\n",
    "    pass0, fail0, pass1, fail1, total = parse_matrix(df)\n",
    "    r_num.append((pass0*fail1)/(total))\n",
    "    r_den.append((fail0*pass1)/(total))\n",
    "print sum(r_num), sum(r_den)\n",
    "print float(sum(r_num))/float(sum(r_den))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CMH Test\n",
    "The `cmh_test` function computes the Cochran-Mantel-Haenszel chi-squared statistic and corresponding p-value. The CMH test statistic grows as the observed values from one group deviates from the pooled expected value of both groups.\n",
    "\n",
    "From McDonald http://www.biostathandbook.com/cmh.html :\n",
    "\n",
    "    \"The numerator contains the absolute value of the difference between the observed value in one cell (a) and the expected value under the null hypothesis, (a+b)(a+c)/n, so the numerator is the squared sum of deviations between the observed and expected values. It doesn't matter how you arrange the 2×2 tables, any of the four values can be used as a. You subtract the 0.5 as a continuity correction. The denominator contains an estimate of the variance of the squared differences.\"\n",
    "\n",
    "    \"The test statistic, χ2MH, gets bigger as the differences between the observed and expected values get larger, or as the variance gets smaller (primarily due to the sample size getting bigger). It is chi-square distributed with one degree of freedom.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.320927767938446, 0.021070789938349432)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from auditai.stats import cmh_test\n",
    "cmh_test(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CMH test statistic is sensitive to sample size at each interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs2 = deepcopy(dfs)\n",
    "dfs2[0] = dfs[0] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.320927767938446, 0.021070789938349432)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmh_test(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29.61407107326871, 5.2720829701868865e-08)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmh_test(dfs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and is primarially suited to identify trends over time, which are magnified at large sample sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[        Estuarine  Marine\n",
       " 94            690     560\n",
       " non-94        770     400,         Estuarine  Marine\n",
       " 94           2570     610\n",
       " non-94       3010     570,         Estuarine  Marine\n",
       " 94            650     730\n",
       " non-94        790     710,         Estuarine  Marine\n",
       " 94            480     710\n",
       " non-94        480     550]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs2 = deepcopy(dfs)\n",
    "for i,d in enumerate(dfs2):\n",
    "    dfs2[i] = d * 10\n",
    "dfs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53.3591828396859, 2.7777780076121417e-13)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmh_test(dfs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breslow-Day Test\n",
    "The `bres_day` function computes the Breslow-Day test of homogeneous association for a 2 x 2 x k table. For example, given three factors, A, B, and C, the Breslow-Day test would measure wheher pairwise effects (AB, AC, BC) have identical odds ratios.\n",
    "\n",
    "Here, we see that the original set of sample data, `dfs`, have relatively consistent directionality of odds ratios of differences between regions and alelle groups. A modified set of data, where the odds ratio of one of the groups is greater and in the opposite direction than odds ratios of other samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfs2 = deepcopy(dfs)\n",
    "dfs2[0].iloc[0,0] = dfs2[0].iloc[0,0] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5294859090315444, 103.09229142737072)\n"
     ]
    }
   ],
   "source": [
    "from auditai.stats import bres_day\n",
    "r = multi_odds_ratio(dfs)\n",
    "part_bd = partial(bres_day, r=r)\n",
    "# sum of Breslow-Day chi-square statistics\n",
    "bd = pd.DataFrame(map(part_bd, dfs))[0].sum()\n",
    "\n",
    "r2 = multi_odds_ratio(dfs2)\n",
    "part_bd2 = partial(bres_day, r=r2)\n",
    "bd2 = pd.DataFrame(map(part_bd, dfs2))[0].sum()\n",
    "\n",
    "print(bd,bd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.156231884058 56.418689433165014 5.861977570020827e-14\n",
      "1.25339613626 4.21275740429837 0.0401210677531979\n",
      "1.24962080173 3.046241637403522 0.0809242118746174\n",
      "1.29090909091 2.6800554756863293 0.1016121884800798\n"
     ]
    }
   ],
   "source": [
    "r2 = multi_odds_ratio(dfs2)\n",
    "for df in dfs2:\n",
    "    r = multi_odds_ratio(df)\n",
    "    bd, pbd = bres_day(df,r2)\n",
    "    print r,bd,pbd\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local",
   "language": "python",
   "name": "local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
